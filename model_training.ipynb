{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy and pandas for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# sklearn preprocessing for dealing with categorical variables\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# modeling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#explainability\n",
    "import shap\n",
    "\n",
    "#serialization\n",
    "import joblib\n",
    "import csv \n",
    "\n",
    "#remove warnings\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data (to be downloaded locally by user of this notebook and store in a parent folder called \"raw_data\")\n",
    "app_train = pd.read_csv(\"../raw_data/application_train.csv\").astype(\"object\")\n",
    "app_test = pd.read_csv(\"../raw_data/application_test.csv\").astype(\"object\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert variables that are not correctly cast as of float in the raw data : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dtypes_func(df):\n",
    "    #List the columns to be converted to float\n",
    "    list_columns_to_convert_to_float = (\"CNT_CHILDREN\" \n",
    "                                ,\"AMT_INCOME_TOTAL\" \n",
    "                                ,\"AMT_CREDIT\" \n",
    "                                ,\"AMT_ANNUITY\" \n",
    "                                ,\"AMT_GOODS_PRICE\" \n",
    "                                ,\"DAYS_BIRTH\"\n",
    "                                ,\"DAYS_EMPLOYED\" \n",
    "                                ,\"DAYS_REGISTRATION\"\n",
    "                                ,\"REGION_RATING_CLIENT\"\n",
    "                                ,\"REGION_POPULATION_RELATIVE\"\n",
    "                                ,\"HOUR_APPR_PROCESS_START\"\n",
    "                                ,\"CNT_FAM_MEMBERS\" \n",
    "                                ,\"DAYS_ID_PUBLISH\"\n",
    "                                ,\"OWN_CAR_AGE\"\n",
    "                                ,\"EXT_SOURCE_1               \"\n",
    "                                ,\"EXT_SOURCE_2               \"\n",
    "                                ,\"EXT_SOURCE_3               \"\n",
    "                                ,\"APARTMENTS_AVG             \"\n",
    "                                ,\"BASEMENTAREA_AVG           \"\n",
    "                                ,\"YEARS_BEGINEXPLUATATION_AVG\"\n",
    "                                ,\"YEARS_BUILD_AVG            \"\n",
    "                                ,\"COMMONAREA_AVG             \"\n",
    "                                ,\"ELEVATORS_AVG              \"\n",
    "                                ,\"ENTRANCES_AVG              \"\n",
    "                                ,\"FLOORSMAX_AVG              \"\n",
    "                                ,\"FLOORSMIN_AVG              \"\n",
    "                                ,\"LANDAREA_AVG               \"\n",
    "                                ,\"LIVINGAPARTMENTS_AVG       \"\n",
    "                                ,\"LIVINGAREA_AVG             \"\n",
    "                                ,\"NONLIVINGAPARTMENTS_AVG    \"\n",
    "                                ,\"NONLIVINGAREA_AVG          \"\n",
    "                                ,\"APARTMENTS_MODE            \"\n",
    "                                ,\"BASEMENTAREA_MODE          \"\n",
    "                                ,\"YEARS_BEGINEXPLUATATION_MODE\"\n",
    "                                ,\"YEARS_BUILD_MODE           \"\n",
    "                                ,\"COMMONAREA_MODE            \"\n",
    "                                ,\"ELEVATORS_MODE             \"\n",
    "                                ,\"ENTRANCES_MODE             \"\n",
    "                                ,\"FLOORSMAX_MODE             \"\n",
    "                                ,\"FLOORSMIN_MODE             \"\n",
    "                                ,\"LANDAREA_MODE              \"\n",
    "                                ,\"LIVINGAPARTMENTS_MODE      \"\n",
    "                                ,\"LIVINGAREA_MODE            \"\n",
    "                                ,\"NONLIVINGAPARTMENTS_MODE   \"\n",
    "                                ,\"NONLIVINGAREA_MODE         \"\n",
    "                                ,\"APARTMENTS_MEDI            \"\n",
    "                                ,\"BASEMENTAREA_MEDI          \"\n",
    "                                ,\"YEARS_BEGINEXPLUATATION_MEDI\"\n",
    "                                ,\"YEARS_BUILD_MEDI           \"\n",
    "                                ,\"COMMONAREA_MEDI            \"\n",
    "                                ,\"ELEVATORS_MEDI             \"\n",
    "                                ,\"ENTRANCES_MEDI             \"\n",
    "                                ,\"FLOORSMAX_MEDI             \"\n",
    "                                ,\"FLOORSMIN_MEDI             \"\n",
    "                                ,\"LANDAREA_MEDI              \"\n",
    "                                ,\"LIVINGAPARTMENTS_MEDI      \"\n",
    "                                ,\"LIVINGAREA_MEDI            \"\n",
    "                                ,\"NONLIVINGAPARTMENTS_MEDI   \"\n",
    "                                ,\"NONLIVINGAREA_MEDI         \"\n",
    "                                ,\"TOTALAREA_MODE             \"\n",
    "                                ,\"OBS_30_CNT_SOCIAL_CIRCLE   \"\n",
    "                                ,\"DEF_30_CNT_SOCIAL_CIRCLE   \"\n",
    "                                ,\"OBS_60_CNT_SOCIAL_CIRCLE   \"\n",
    "                                ,\"DEF_60_CNT_SOCIAL_CIRCLE   \"\n",
    "                                ,\"DAYS_LAST_PHONE_CHANGE     \"\n",
    "                                ,\"AMT_REQ_CREDIT_BUREAU_HOUR\"\n",
    "                                ,\"AMT_REQ_CREDIT_BUREAU_DAY \"\n",
    "                                ,\"AMT_REQ_CREDIT_BUREAU_WEEK\"\n",
    "                                ,\"AMT_REQ_CREDIT_BUREAU_MON \"\n",
    "                                ,\"AMT_REQ_CREDIT_BUREAU_QRT \"\n",
    "                                ,\"AMT_REQ_CREDIT_BUREAU_YEAR\")       \n",
    "\n",
    "    #remove spaces in the list created\n",
    "    list_columns_to_convert_to_float = [s.strip() for s in list_columns_to_convert_to_float]       \n",
    "    \n",
    "    convert_count = 0\n",
    "\n",
    "    #convert object columns to float\n",
    "    for col in list_columns_to_convert_to_float:\n",
    "        df[col] = df[col].astype(float)\n",
    "        \n",
    "        # Keep track of how many columns were label encoded\n",
    "        convert_count += 1\n",
    "\n",
    "    print('%d object columns were converted to int.' % convert_count)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_eng(df):\n",
    "\n",
    "    # Create an anomalous flag column\n",
    "    df['DAYS_EMPLOYED_ANOM'] = df[\"DAYS_EMPLOYED\"] == 365243\n",
    "\n",
    "    # Replace the anomalous values with nan\n",
    "    df['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n",
    "\n",
    "    #correct sign of Days Birth\n",
    "    df[\"DAYS_BIRTH\"] = abs(df[\"DAYS_BIRTH\"])\n",
    "    df['AGE_INT'] = round((df['DAYS_BIRTH'] / 365).astype(float),2)\n",
    "\n",
    "    #footing financial ratios\n",
    "    df['annuity_income_ratio'] = round((df['AMT_INCOME_TOTAL'] / df['AMT_ANNUITY']).astype(float),2)\n",
    "    df['credit_annuity_ratio'] = round((df['AMT_CREDIT'] / df['AMT_ANNUITY']).astype(float),2)\n",
    "    df['credit_goods_price_ratio'] = round((df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']).astype(float),2)\n",
    "    df['credit_downpayment'] = round((df['AMT_GOODS_PRICE'] - df['AMT_CREDIT']).astype(float),2)\n",
    " \n",
    "    print('Feature engineering success')\n",
    "   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_feature_selection(df):\n",
    "    columns_list = df.columns.to_list()\n",
    "\n",
    "    columns_to_drop_intersec = (['DAYS_BIRTH',\n",
    "    'REGION_RATING_CLIENT_W_CITY',\n",
    "    'BASEMENTAREA_MODE',\n",
    "    'YEARS_BUILD_MODE',\n",
    "    'COMMONAREA_MODE',\n",
    "    'ELEVATORS_MODE',\n",
    "    'ENTRANCES_MODE',\n",
    "    'FLOORSMAX_MODE',\n",
    "    'LANDAREA_MODE',\n",
    "    'LIVINGAPARTMENTS_MODE',\n",
    "    'NONLIVINGAPARTMENTS_MODE',\n",
    "    'NONLIVINGAREA_MODE',\n",
    "    'APARTMENTS_MEDI',\n",
    "    'BASEMENTAREA_MEDI',\n",
    "    'YEARS_BEGINEXPLUATATION_MEDI',\n",
    "    'YEARS_BUILD_MEDI',\n",
    "    'COMMONAREA_MEDI',\n",
    "    'ELEVATORS_MEDI',\n",
    "    'ENTRANCES_MEDI',\n",
    "    'FLOORSMAX_MEDI',\n",
    "    'FLOORSMIN_MEDI',\n",
    "    'LIVINGAPARTMENTS_MEDI',\n",
    "    'LIVINGAREA_MEDI',\n",
    "    'NONLIVINGAPARTMENTS_MEDI',\n",
    "    'NONLIVINGAREA_MEDI',\n",
    "    'OBS_60_CNT_SOCIAL_CIRCLE',\n",
    "    \"DAYS_EMPLOYED_ANOM\"])\n",
    "\n",
    "    #Drop columns\n",
    "    df_selec_col = pd.DataFrame(data=df, columns=columns_list).drop(columns_to_drop_intersec, axis=1)\n",
    "    \n",
    "    print('Feature selection success')\n",
    "\n",
    "    return df_selec_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation - Encoding - Standardization - Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 object columns were converted to int.\n",
      "Feature engineering success\n",
      "Feature selection success\n"
     ]
    }
   ],
   "source": [
    "app_train = convert_dtypes_func(app_train)\n",
    "app_train = feature_eng(app_train)\n",
    "app_train = df_feature_selection(app_train)\n",
    "\n",
    "#create df of features by type\n",
    "cat_features = app_train.select_dtypes(include=['object']).drop(['SK_ID_CURR', 'TARGET'], axis=1)\n",
    "num_features = app_train.select_dtypes(exclude=['object'])\n",
    "\n",
    "#imputation\n",
    "categorical_imputer = SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")\n",
    "cat_features = categorical_imputer.fit_transform(cat_features)\n",
    "\n",
    "simple_imputer = SimpleImputer(missing_values=np.nan, strategy=\"median\")\n",
    "num_features = simple_imputer.fit_transform(num_features)\n",
    "\n",
    "#One hot encoding categorical variables\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "cat_array = ohe.fit_transform(cat_features).todense()\n",
    "cat_array = np.asarray(cat_array)\n",
    "\n",
    "#Standard Scaling numerical variables\n",
    "scaler = StandardScaler()\n",
    "num_array = scaler.fit_transform(num_features)\n",
    "\n",
    "#concatenate\n",
    "X_train = np.concatenate([cat_array, num_array], axis=1)\n",
    "y_train = app_train['TARGET'].astype(int)\n",
    "\n",
    "#columns list after ohe\n",
    "cat_features_list = list(app_train.select_dtypes(include=['object']).drop(['SK_ID_CURR','TARGET'], axis=1).columns)\n",
    "#https://stackoverflow.com/questions/54570947/feature-names-from-onehotencoder\n",
    "cat_features_list_after_ohe = ohe.get_feature_names(cat_features_list).tolist()\n",
    "num_features_list_after_preproc = list(app_train.select_dtypes(exclude=['object']).columns)\n",
    "\n",
    "#concatenate list of features\n",
    "features_list_after_prepr = cat_features_list_after_ohe + num_features_list_after_preproc\n",
    "\n",
    "#transform X into a dataframe with column names\n",
    "ohe_dataframe = pd.DataFrame(X_train, columns=features_list_after_prepr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 object columns were converted to int.\n",
      "Feature engineering success\n",
      "Feature selection success\n"
     ]
    }
   ],
   "source": [
    "app_test = convert_dtypes_func(app_test)\n",
    "app_test = feature_eng(app_test)\n",
    "app_test = df_feature_selection(app_test)\n",
    "\n",
    "#create df of features by type\n",
    "cat_features = app_test.select_dtypes(include=['object']).drop(['SK_ID_CURR'], axis=1)\n",
    "num_features = app_test.select_dtypes(exclude=['object'])\n",
    "\n",
    "#imputation\n",
    "cat_features = categorical_imputer.transform(cat_features)\n",
    "num_features = simple_imputer.transform(num_features)\n",
    "\n",
    "#One hot encoding categorical variables\n",
    "cat_array = ohe.transform(cat_features).todense()\n",
    "cat_array = np.asarray(cat_array)\n",
    "\n",
    "#Standard Scaling numerical variables\n",
    "num_array = scaler.transform(num_features)\n",
    "\n",
    "#concatenate\n",
    "X_test = np.concatenate([cat_array, num_array], axis=1)\n",
    "\n",
    "#columns list after ohe\n",
    "cat_features_list_test = list(app_test.select_dtypes(include=['object']).drop(['SK_ID_CURR'], axis=1).columns)\n",
    "cat_features_list_after_ohe_test = ohe.get_feature_names(cat_features_list_test).tolist()\n",
    "num_features_list_after_preproc_test = list(app_test.select_dtypes(exclude=['object']).columns)\n",
    "\n",
    "#concatenate list of features\n",
    "features_list_after_prepr_test = cat_features_list_after_ohe_test + num_features_list_after_preproc_test\n",
    "\n",
    "#transform X into a dataframe with column names\n",
    "ohe_dataframe_test = pd.DataFrame(X_test, columns=features_list_after_prepr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, class_weight='balanced', max_iter=1000)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model with best params\n",
    "model = LogisticRegression(class_weight = 'balanced', C=0.01, max_iter=1000)\n",
    "\n",
    "#fit\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "# #predict\n",
    "# pred = model.predict_proba(X_test)\n",
    "# y_pred = pred[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app_test[\"pred\"] = y_pred\n",
    "# app_test[\"pred\"] = round(app_test[\"pred\"].astype(np.float64),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict_proba returns the probability of the sample\n",
    "#for each class in the model\n",
    "#where classes are ordered as they are in self.classes_.\n",
    "model.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Footing Shapley Explainer on Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_sampled_train_data = shap.sample(ohe_dataframe, 50)\n",
    "# log_reg_explainer = shap.KernelExplainer(model.predict_proba, sub_sampled_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Shap Expected Value : {log_reg_explainer.expected_value}')\n",
    "\n",
    "# result_proba_training = model.predict_proba(X_train).mean(axis=0)\n",
    "# print(f'Model Mean Value (Theoretical Expected Value of default) : {result_proba_training[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on Test Data and Shap Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dict = joblib.load(\"./bin/data_dict.joblib\")\n",
    "# ohe = joblib.load(\"./bin/ohe.joblib\")\n",
    "# categorical_imputer = joblib.load(\"./bin/categorical_imputer.joblib\")\n",
    "# simple_imputer = joblib.load(\"./bin/simple_imputer.joblib\")\n",
    "# scaler = joblib.load(\"./bin/scaler.joblib\")\n",
    "# model = joblib.load(\"./bin/model.joblib\")\n",
    "\n",
    "# #SimpleImputing (most frequent) and ohe of categorical features\n",
    "# cat_array = categorical_imputer.transform(df_test[list_cat_features])\n",
    "# cat_array = ohe.transform(cat_array).todense()\n",
    "\n",
    "# #SimpleImputing (median) and StandardScaling of numerical features\n",
    "# num_array = simple_imputer.transform(df_test[list_num_features])\n",
    "# num_array = scaler.transform(num_array)\n",
    "\n",
    "# #concatenate\n",
    "# X = np.concatenate([cat_array, num_array], axis=1)\n",
    "# X = np.asarray(X)\n",
    "\n",
    "# #predict\n",
    "# result_proba = model.predict_proba(X)\n",
    "# y_pred_proba = result_proba[:,1]\n",
    "\n",
    "# #cat columns list after ohe\n",
    "# cat_features_list_after_ohe = ohe.get_feature_names(list_cat_features).tolist()\n",
    "\n",
    "# #concatenate list of features\n",
    "# features_list_after_prepr_test = cat_features_list_after_ohe + list_num_features\n",
    "\n",
    "# #transform X into a dataframe with column names\n",
    "# ohe_dataframe_test = pd.DataFrame(X, columns=features_list_after_prepr_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_idx = 0\n",
    "# sub_sampled_test_data = ohe_dataframe.iloc[sample_idx,:].values.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap_vals = log_reg_explainer.shap_values(sub_sampled_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Model prediction for test data\", model.predict_proba(sub_sampled_test_data))\n",
    "\n",
    "# shap.initjs()\n",
    "# #https://shap-lrjball.readthedocs.io/en/latest/generated/shap.force_plot.html\n",
    "# shap.force_plot(base_value=log_reg_explainer.expected_value[1],\n",
    "#                 shap_values=shap_vals[1][0],\n",
    "#                 features=sub_sampled_test_data[0],\n",
    "#                 feature_names=features_list_after_prepr_test\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #https://shap-lrjball.readthedocs.io/en/latest/generated/shap.summary_plot.html\n",
    "# shap.summary_plot(shap_values=shap_vals, \n",
    "#                   features=sub_sampled_test_data, \n",
    "#                   feature_names=features_list_after_prepr_test, \n",
    "#                   max_display=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap.plots._waterfall.waterfall_legacy(log_reg_explainer.expected_value[1],#expected_value,\n",
    "#                                        shap_vals[1][0],\n",
    "#                                        sub_sampled_test_data[0],\n",
    "#                                        feature_names=features_list_after_prepr_test,\n",
    "#                                        max_display=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export pre-processed dataset to be used for Dashboard Streamlit (on which heroku deployed model will predicting scoring) : \n",
    "- Sample 5% of the labelled dataset (app_train)\n",
    "- Sample 5% of the unlabelled dataset (app_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train exported to dashboard_data folder.\n",
      "df_test exported to dashboard_data folder.\n"
     ]
    }
   ],
   "source": [
    "#labelled dataset\n",
    "df_train = app_train.sample(frac=0.05, random_state=0)\n",
    "df_train.to_csv(\"./dashboard_data/df_train.csv\", index=False)\n",
    "print('df_train exported to dashboard_data folder.')\n",
    "\n",
    "#Unlabelled dataset\n",
    "df_test = app_test.sample(frac=0.05, random_state=0)\n",
    "df_test.to_csv(\"./dashboard_data/df_test.csv\", index=False)\n",
    "\n",
    "#export datasets of variables per type in order to prevent features type \n",
    "#change during csv reading in api.py\n",
    "df_test_cat_features = df_test.select_dtypes(include=[\"object\"]).drop([\"SK_ID_CURR\"], axis=1)\n",
    "df_test_num_features = df_test.select_dtypes(exclude=['object'])\n",
    "\n",
    "df_test_cat_features.to_csv(\"./dashboard_data/df_test_cat_features.csv\", index=False)\n",
    "df_test_num_features.to_csv(\"./dashboard_data/df_test_num_features.csv\", index=False)\n",
    "\n",
    "print('df_test exported to dashboard_data folder.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bin/model.joblib']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(ohe, 'bin/ohe.joblib') #into a folder bin (for binary)\n",
    "joblib.dump(categorical_imputer, 'bin/categorical_imputer.joblib')\n",
    "joblib.dump(simple_imputer, 'bin/simple_imputer.joblib')\n",
    "joblib.dump(scaler, 'bin/scaler.joblib')\n",
    "joblib.dump(model, 'bin/model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pydantic Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pydantic import BaseModel, create_model, Field, ValidationError\n",
    "\n",
    "# model = {}\n",
    "# for i in range(len(app_train.dtypes)):\n",
    "#     name = app_train.dtypes.index[i]\n",
    "#     var_type = type(app_train.iloc[0, i])\n",
    "#     model.update({name: (var_type, Field(...))})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model[\"SK_ID_CURR\"]\n",
    "# del model[\"TARGET\"]\n",
    "# # del model[\"DAYS_EMPLOYED_ANOM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dict = joblib.dump(model, 'bin/data_dict.joblib')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
